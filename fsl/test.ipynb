{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5febb403-c61e-4ae8-8976-ecab9bcc5bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture --no-stderr\n",
    "# !pip install git+https://github.com/openai/CLIP timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99cca730-6395-49ad-a050-3131d3573df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from igniter.builder import build_engine\n",
    "from fsl.models.meta_arch import build_sam_fsod\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e35d2c2-0a87-449c-91ec-1a894f366e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "\u001b[33m2023-11-15 16:53:11,493 [builder.py:252] WARNING: # TODO: Remove hardcoded name and replace with registry based\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.09s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading ...7_5shot/model_0000009.pt: 100%|██████████████████████████████████████████████| 420M/420M [01:23<00:00, 5.28MB/s]\n"
     ]
    }
   ],
   "source": [
    "config_file = '../configs/devit/resnet_trainval_30shot.yaml'\n",
    "# config_file = '../configs/devit/sam_vitb_trainval_30shot.yaml'\n",
    "\n",
    "cfg = OmegaConf.load(config_file)\n",
    "# print(OmegaConf.to_yaml(cfg))\n",
    "\n",
    "cfg.datasets.dataloader.batch_size = 1\n",
    "cfg.datasets.dataloader.num_workers = 1\n",
    "cfg.datasets.dataloader.shuffle=False\n",
    "cfg.options.train = False\n",
    "cfg.options.eval = True\n",
    "cfg.build.resnet_fsod.weights=\"s3://sr-shokunin/perception/models/fsl/resnet_vitb_coco17_30shot/model_0000001.pt\"\n",
    "# cfg.build.sam_fsod.weights = \"s3://sr-shokunin/perception/models/fsl/sam_vitb_coco17_30shot/model_0000005.pt\"\n",
    "cfg.build.dinov2_fsod.weights = \"s3://sr-shokunin/perception/models/fsl/dinov2_vitb_coco17_5shot/model_0000009.pt\"\n",
    "\n",
    "cfg.build.model = \"dinov2_fsod\"\n",
    "\n",
    "engine = build_engine(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5ca4f17-e960-436d-beab-9806d02bd126",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = max([len(iid) for iid in engine._model.classifier._all_cids]) + 1\n",
    "gt_labels = lambda indices: [engine._model.classifier._all_cids[i] for i in indices]\n",
    "\n",
    "def accuracy(scores, gt_names, topk = 1):\n",
    "    _, indices = torch.topk(scores, topk, dim=1)\n",
    "    pred_labels = [gt_labels(index) for index in indices]\n",
    "\n",
    "    count = 0.0\n",
    "    for gt_name, labels in zip(gt_names, pred_labels):\n",
    "        count += 1 if gt_name in labels else 0\n",
    "    return count / len(gt_names)\n",
    "\n",
    "def print_labels(scores, targets, k = 2):\n",
    "    gt_names = targets[0]['gt_proposal'].labels\n",
    "    _, indexes = torch.topk(scores, k, dim=1)\n",
    "    \n",
    "    for name, i in zip(gt_names, indexes.cpu().numpy()):\n",
    "        string = name.ljust(size) + \" | \" + \"\".join([n.ljust(size) for n in gt_labels(i)])\n",
    "        print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9111b4d-a334-48fd-af46-372d2be5487f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Top 1: 0.304 | Top 5 0.591:  58%|█████████████████████████████████▍                        | 2403/4167 [31:24<19:49,  1.48it/s]"
     ]
    }
   ],
   "source": [
    "acc_top1, acc_top5 = 0.0, 0.0\n",
    "progress_bar = tqdm(total=len(engine._dataloader), desc=\"Processing\", position=0, leave=True)\n",
    "\n",
    "for k, (images, targets) in enumerate(engine._dataloader, 1):\n",
    "    with torch.no_grad():\n",
    "        output, _ = engine._model(images, targets)\n",
    "    scores = output['scores']\n",
    "    # gt_names = targets[0]['gt_proposal'].labels\n",
    "    gt_names = [target['gt_proposal'].labels for target in targets]\n",
    "    gt_names = [item for sublist in gt_names for item in sublist]\n",
    "\n",
    "    t1 = accuracy(scores, gt_names, topk=1)\n",
    "    t5 = accuracy(scores, gt_names, topk=5)\n",
    "\n",
    "    acc_top1 += (t1 - acc_top1) / k\n",
    "    acc_top5 += (t5 - acc_top5) / k\n",
    "\n",
    "    # print(\"\\nImage ID: \", targets[0]['gt_proposal'].image_id)\n",
    "    # print_labels(scores, targets, k=5)\n",
    "    progress_bar.set_description(f\"Top 1: {acc_top1:.3f} | Top 5 {acc_top5:.3f}\")\n",
    "    progress_bar.update(1)\n",
    "\n",
    "progress_bar.close()\n",
    "\n",
    "print(\"\\nTop 1: \", acc_top1, \"Top 5: \", acc_top5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03ed8ba9-59e5-4423-b7e4-b53978460d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 1:  0.43006454187764404 Top 5:  0.659023426230131\n",
    "# Top 1:  0.4290284612805177 Top 5:  0.659921149440774"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
