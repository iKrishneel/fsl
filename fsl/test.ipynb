{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5febb403-c61e-4ae8-8976-ecab9bcc5bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture --no-stderr\n",
    "# !pip install git+https://github.com/openai/CLIP timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99cca730-6395-49ad-a050-3131d3573df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from igniter.builder import build_engine\n",
    "from igniter.main import get_full_config\n",
    "\n",
    "from fsl.models.meta_arch import build_sam_fsod\n",
    "from fsl.models.meta_arch import *\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e35d2c2-0a87-449c-91ec-1a894f366e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "\u001b[33m2023-11-15 21:35:49,697 [attention.py:25] WARNING: xFormers not available\u001b[0m\n",
      "\u001b[33m2023-11-15 21:35:49,698 [block.py:32] WARNING: xFormers not available\u001b[0m\n",
      "\u001b[33m2023-11-15 21:35:50,735 [builder.py:255] WARNING: # TODO: Remove hardcoded name and replace with registry based\u001b[0m\n",
      "/root/krishneel/Documents/github/igniter/igniter/datasets/s3_dataset.py:55: UserWarning: Target transforms is not yet implemented\n",
      "  warnings.warn('Target transforms is not yet implemented')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.18s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ignite/contrib/handlers/tqdm_logger.py:126: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "config_file = '../configs/devit/resnet_trainval_30shot.yaml'\n",
    "config_file = '../configs/devit/devit_dinov2_trainval_xshot.yaml'\n",
    "\n",
    "cfg = get_full_config(config_file)\n",
    "# cfg = OmegaConf.load(config_file)\n",
    "# print(OmegaConf.to_yaml(cfg))\n",
    "\n",
    "cfg.datasets.dataloader.batch_size = 1\n",
    "cfg.datasets.dataloader.num_workers = 1\n",
    "cfg.datasets.dataloader.shuffle=False\n",
    "cfg.options.train = False\n",
    "cfg.options.eval = True\n",
    "# cfg.build.resnet_fsod.weights=\"s3://sr-shokunin/perception/models/fsl/resnet_vitb_coco17_30shot/model_0000001.pt\"\n",
    "# cfg.build.sam_fsod.weights = \"s3://sr-shokunin/perception/models/fsl/sam_vitb_coco17_30shot/model_0000005.pt\"\n",
    "# cfg.build.dinov2_fsod.weights = \"s3://sr-shokunin/perception/models/fsl/dinov2_vitb_coco17_5shot/model_0000009.pt\"\n",
    "\n",
    "# cfg.build.model = \"dinov2_fsod\"\n",
    "\n",
    "engine = build_engine(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ee93157-9109-41aa-85bd-d7256a367123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FSOD(\n",
       "  (mask_generator): DinoV2Patch(\n",
       "    (backbone): DinoVisionTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-11): 12 x NestedTensorBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MemEffAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): LayerScale()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (head): Identity()\n",
       "    )\n",
       "  )\n",
       "  (classifier): DeVit(\n",
       "    (fc_bg_class): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (fc_back_class): Linear(in_features=530, out_features=128, bias=True)\n",
       "    (bg_cnn): PropagateNet(\n",
       "      (main_layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2d(257, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): Conv2d(258, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (mask_layers): ModuleList(\n",
       "        (0-2): 3 x Conv2d(256, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "      (class_proj): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "    (fc_other_class): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (fc_intra_class): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (per_cls_cnn): PropagateNet(\n",
       "      (main_layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2d(257, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): Conv2d(258, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (mask_layers): ModuleList(\n",
       "        (0-2): 3 x Conv2d(256, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "      (class_proj): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (roi_pooler): RoIAlign(output_size=7, spatial_scale=0.07142857142857142, sampling_ratio=-1, aligned=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine._model.eval().to(torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5ca4f17-e960-436d-beab-9806d02bd126",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = max([len(iid) for iid in engine._model.classifier._all_cids]) + 1\n",
    "gt_labels = lambda indices: [engine._model.classifier._all_cids[i] for i in indices]\n",
    "\n",
    "def accuracy(scores, gt_names, topk = 1):\n",
    "    _, indices = torch.topk(scores, topk, dim=1)\n",
    "    pred_labels = [gt_labels(index) for index in indices]\n",
    "\n",
    "    count = 0.0\n",
    "    for gt_name, labels in zip(gt_names, pred_labels):\n",
    "        count += 1 if gt_name in labels else 0\n",
    "    return count / len(gt_names)\n",
    "\n",
    "def print_labels(scores, targets, k = 2):\n",
    "    gt_names = targets[0]['gt_proposal'].labels\n",
    "    _, indexes = torch.topk(scores, k, dim=1)\n",
    "    \n",
    "    for name, i in zip(gt_names, indexes.cpu().numpy()):\n",
    "        string = name.ljust(size) + \" | \" + \"\".join([n.ljust(size) for n in gt_labels(i)])\n",
    "        print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9111b4d-a334-48fd-af46-372d2be5487f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|                                                                                     | 0/4167 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "Top 1: 0.801 | Top 5 0.893: 100%|████████████████████████████████████████████████████████| 4167/4167 [1:47:24<00:00,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 1:  0.8010184982734054 Top 5:  0.8928020449343591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "acc_top1, acc_top5 = 0.0, 0.0\n",
    "progress_bar = tqdm(total=len(engine._dataloader), desc=\"Processing\", position=0, leave=True)\n",
    "\n",
    "for k, (images, targets) in enumerate(engine._dataloader, 1):\n",
    "    with torch.no_grad():\n",
    "        output, _ = engine._model(images, targets)\n",
    "    scores = output['scores']\n",
    "    # gt_names = targets[0]['gt_proposal'].labels\n",
    "    gt_names = [target['gt_proposal'].labels for target in targets]\n",
    "    gt_names = [item for sublist in gt_names for item in sublist]\n",
    "\n",
    "    t1 = accuracy(scores, gt_names, topk=1)\n",
    "    t5 = accuracy(scores, gt_names, topk=5)\n",
    "\n",
    "    acc_top1 += (t1 - acc_top1) / k\n",
    "    acc_top5 += (t5 - acc_top5) / k\n",
    "\n",
    "    # print(\"\\nImage ID: \", targets[0]['gt_proposal'].image_id)\n",
    "    # print_labels(scores, targets, k=5)\n",
    "    progress_bar.set_description(f\"Top 1: {acc_top1:.3f} | Top 5 {acc_top5:.3f}\")\n",
    "    progress_bar.update(1)\n",
    "\n",
    "progress_bar.close()\n",
    "\n",
    "print(\"\\nTop 1: \", acc_top1, \"Top 5: \", acc_top5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03ed8ba9-59e5-4423-b7e4-b53978460d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 1:  0.43006454187764404 Top 5:  0.659023426230131\n",
    "# Top 1:  0.4290284612805177 Top 5:  0.659921149440774\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
